# -*- coding: utf-8 -*-
"""NgrokMario MorelosM7. Técnicas avanzadas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zRMJ9i9YyoyJrcaRuS9xgYrQC7iB897

## **Bootcamp: Ciencia de Datos e Inteligencia Artificial**
## **Proyecto del Módulo 7: Técnicas avanzadas para ciencia de datos y empleabilidad**

Hola, ya es el último proyecto, has avanzado y aprendido mucho hasta acá. ¡Muchas felicidades!

Es hora de poner en práctica todo lo que hemos aprendido a lo largo de nuestra travesía.

Lee el proyecto y revisa con cuidado cada una de las instrucciones. Procura plasmar todo tu potencial para que lo concluyas de manera sobresaliente.

¡Éxito!

# Objetivos
- Aplicar con éxito todos los conocimientos que has adquirido a lo largo del Bootcamp.
- Consolidar las técnicas de limpieza, entrenamiento, graficación y ajuste a modelos de *Machine Learning*.
- Generar una API que brinde predicciones como resultado a partir de datos enviados.

# Proyecto

1. Selecciona uno de los siguientes *datasets*:
  - *Reviews* de aplicaciones de la Google Play Store: https://www.kaggle.com/datasets/lava18/google-play-store-apps
  - Estadísticas demográficas de los ganadores del premio Oscar de la Academia: https://www.kaggle.com/datasets/fmejia21/demographics-of-academy-awards-oscars-winners
  - Aspiraciones profesionales de la generación Z: https://www.kaggle.com/datasets/kulturehire/understanding-career-aspirations-of-genz

Cada uno representa un *dataset*, un problema y una forma diferente de abordarlo. Tu tarea es identificar las técnicas y modelos que podrías usar para tu proyecto.

2. Debes hacer un análisis exploratorio y limpieza de los datos. Usa las ténicas que creas convenientes.

3. Entrena el modelo de *Machine Learning*, procesamiento de lenguaje natural o red neuronal que creas adecuado.

4. Genera por lo menos dos gráficas y dos métricas de rendimiento; explica las puntuaciones de rendimiento que amerite tu problema. Todas las gráficas de rendimiento que realices deben tener leyendas, colores y títulos personalizados por ti.

  - Además, antes de subir el modelo a "producción", deberás realizar un proceso de ensambles (*ensemblings*) y de ajuste de hiperparámetros o *tuning* para intentar mejorar la precisión y disminuir la varianza de tu modelo.

5. Construye una API REST en la que cualquier usuario pueda mandar datos y que esta misma devuelva la predicción del modelo que has hecho. La API debe estar en la nube, ya sea en un servicio como Netlify o Ngrok, para que pueda ser consultada desde internet.

6. Genera una presentación del problema y del modelo de solución que planteas. Muestra gráficas, datos de rendimiento y explicaciones. Esta presentación debe estar enfocada a personas que no sepan mucho de ciencia de datos e inteligencia artificial.

7. **Solamente se recibirán trabajos subidos a tu cuenta de GitHub con un README.md apropiado que explique tu proyecto**.

## Criterios de evaluación

| Actividad | Porcentaje | Observaciones | Punto parcial
| -- | -- | -- | -- |
| Actividad 1. Limpieza y EDA | 20 | Realiza todas las tareas necesarias para hacer el EDA y la limpieza correcta, dependiendo de la problemática. Debes hacer como mínimo el análisis de completitud, escalamiento (si aplica) y tokenización (si aplica). | Realizaste solo algunas tareas de exploración y limpieza y el modelo se muestra aún con oportunidad de completitud, escalamiento y/o mejora. |
| Actividad 2. Entrenamiento del modelo | 20 | Elige el modelo y algoritmo adecuados para tu problema, entrénalo con los datos ya limpios y genera algunas predicciones de prueba. | No has realizado predicciones de prueba para tu modelo de ML y/o tu modelo muestra una precisión menor al 60 %. |
| Actividad 3. Graficación y métricas | 20 | Genera por lo menos dos gráficas y dos muestras de métricas que permitan visualizar el rendimiento y precisión del modelo que construiste. Además, realizaste los procesos de *tuning* y ensambles adecuados para tu problema. | Las gráficas no tienen leyendas y colores customizados, solo muestras una gráfica o no realizaste el *tuning* de hiperparámetros.
| Actividad 4. API REST | 20 | Generaste con éxito un *link* público en el que, por método POST, se puede mandar información y la API REST devuelve una predicción junto con el porcentaje de confianza de esta misma. | N/A
| Actividad 5. Presentación | 20 | Genera una presentación en la que establezcas como mínimo: el problema, proceso de solución, metodologías usadas, gráficas de rendimiento, demostración del modelo y aprendizajes obtenidos. Debes redactarla con términos que pueda entender cualquier persona, no solo científicos de datos. | La presentación no expone con claridad o en términos coloquiales el proceso de creación del modelo, sus ventajas y muestras de rendimiento.

**Mucho éxito en tu camino como Data Scientist.**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

FinalProject = pd.read_csv('/content/drive/MyDrive/UCAMP 7 Final/googleplaystore_user_reviews.csv')

FinalProject.head(5)

print(FinalProject.head())

# Se obtiene un resumen de los datos #
print(FinalProject.describe())

# Verificamos valores nulos en cada columna #

print(FinalProject.isnull().sum())

# Para simplificar el analisis eliminamos las filas donde Translated_Review esté vacío ya que no aporta nada útil al modelo #
FinalProject_cleaned = FinalProject.dropna(subset=['Translated_Review'])

# Comprobar que se eliminaron las filas #
print(FinalProject_cleaned.isnull().sum())

# Llenar valores nulos en 'Sentiment_Polarity' y 'Sentiment_Subjectivity' con la mediana
FinalProject_cleaned['Sentiment_Polarity'].fillna(FinalProject_cleaned['Sentiment_Polarity'].median(), inplace=True)
FinalProject_cleaned['Sentiment_Subjectivity'].fillna(FinalProject_cleaned['Sentiment_Subjectivity'].median(), inplace=True)

# Verificar nuevamente si hay valores nulos
print(FinalProject_cleaned.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

# Contar el número de instancias de cada sentimiento
sentiment_counts = FinalProject_cleaned['Sentiment'].value_counts()

# Graficar la distribución de los sentimientos
plt.figure(figsize=(10, 6))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')
plt.title('Distribución de Sentimientos')
plt.xlabel('Sentimiento')
plt.ylabel('Número de Reseñas')
plt.show()

# Agregué color en ésta gráfica #

# Distribución de Sentiment_Polarity
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(FinalProject_cleaned['Sentiment_Polarity'], kde=True, color='blue')
plt.title('Distribución de Sentiment Polarity')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Frecuencia')

# Distribución de Sentiment_Subjectivity
plt.subplot(1, 2, 2)
sns.histplot(FinalProject_cleaned['Sentiment_Subjectivity'], kde=True, color='green')
plt.title('Distribución de Sentiment Subjectivity')
plt.xlabel('Sentiment Subjectivity')
plt.ylabel('Frecuencia')

plt.tight_layout()
plt.show()

# Gracias a éstas graficas comparativas vemos claramente la diferencia entre Sentiment_Polarity y Sentiment_Subjectivity #

# Correlación entre Sentiment_Polarity y Sentiment_Subjectivity
# la forma de la gráfica se me hizo interesante en cuanto a su forma
# y confirma la correlación, la polaridad está en el rango de [-1,1] donde -1 es negativo y 1 es positivo
# y la subjetividad de acuerdo al estudio es el score de los sentimientos, creencias, etc. de los usuarios #
plt.figure(figsize=(10, 6))
sns.scatterplot(x="Sentiment_Polarity", y="Sentiment_Subjectivity", hue="Sentiment", data=FinalProject_cleaned)
plt.title('Correlación entre Sentiment Polarity y Sentiment Subjectivity')
plt.show()

FinalProject_cleaned.to_csv('FinalProject_cleaned.csv', index=False)

"""Entrena el modelo de Machine Learning, procesamiento de lenguaje natural o red neuronal que creas adecuado."""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Tokenización y secuenciación
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(FinalProject_cleaned['Translated_Review'])

# Convertimos los textos a secuencias
sequences = tokenizer.texts_to_sequences(FinalProject_cleaned['Translated_Review'])
padded_sequences = pad_sequences(sequences, padding='post')

# Codificar etiquetas de sentimiento
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(FinalProject_cleaned['Sentiment'])

# División de datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Verificar las formas de los datos
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# Construcción del modelo
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=padded_sequences.shape[1]),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')  # 3 clases: positivo, negativo, neutral
])

# Compilación del modelo
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Resumen del modelo
model.summary()

# Entrenamiento del modelo
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=2)

# Evaluación del modelo
loss, accuracy = model.evaluate(X_test, y_test, verbose=2)
print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

# Precisión y matriz de confusión
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report # Import the necessary functions

y_pred = model.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)
accuracy = accuracy_score(y_test, y_pred_classes)
conf_matrix = confusion_matrix(y_test, y_pred_classes)
class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)

print(f'Precisión del modelo: {accuracy}')
print('Matriz de confusión:')
print(conf_matrix)
print('Reporte de clasificación:')
print(class_report)

"""Genera por lo menos dos gráficas y dos métricas de rendimiento; explica las puntuaciones de rendimiento que amerite tu problema. Todas las gráficas de rendimiento que realices deben tener leyendas, colores y títulos personalizados por ti."""

# Precisión y matriz de confusión
y_pred = model.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)
accuracy = accuracy_score(y_test, y_pred_classes)
conf_matrix = confusion_matrix(y_test, y_pred_classes)
class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)

print(f'Precisión del modelo: {accuracy}')
print('Matriz de confusión:')
print(conf_matrix)
print('Reporte de clasificación:')
print(class_report)

# Gráfica de precisión
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Precisión en entrenamiento', color='blue', linestyle='dashed', linewidth=2)
plt.plot(history.history['val_accuracy'], label='Precisión en validación', color='green', linestyle='solid', linewidth=2)
plt.xlabel('Época', fontsize=14)
plt.ylabel('Precisión', fontsize=14)
plt.title('Evolución de la precisión del modelo', fontsize=16)
plt.legend(loc='lower right', fontsize=12)
plt.grid(True)
plt.show()

# Gráfica de pérdida
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Pérdida en entrenamiento', color='red', linestyle='dashed', linewidth=2)
plt.plot(history.history['val_loss'], label='Pérdida en validación', color='orange', linestyle='solid', linewidth=2)
plt.xlabel('Época', fontsize=14)
plt.ylabel('Pérdida', fontsize=14)
plt.title('Evolución de la pérdida del modelo', fontsize=16)
plt.legend(loc='upper right', fontsize=12)
plt.grid(True)
plt.show()

# Gráfica de la matriz de confusión
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicción', fontsize=14)
plt.ylabel('Actual', fontsize=14)
plt.title('Matriz de confusión', fontsize=16)
plt.show()

"""Además, antes de subir el modelo a "producción", deberás realizar un proceso de ensambles (ensemblings) y de ajuste de hiperparámetros o tuning para intentar mejorar la precisión y disminuir la varianza de tu modelo."""

from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer
from sklearn.linear_model import LogisticRegression # Import LogisticRegression


# Paso 2: Preprocesamiento del texto (ya implementado anteriormente)


# Paso 2: Transformación de texto a características usando TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(FinalProject_cleaned['Translated_Review']).toarray()
y = FinalProject_cleaned['Sentiment'].apply(lambda x: 1 if x == 'Positive' else 0 if x == 'Negative' else 2)


# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Paso 3: Entrenamiento del modelo
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Paso 4: Evaluación del modelo
y_pred = model.predict(X_test)


# Definir el modelo de RandomForestClassifier
rf_model = RandomForestClassifier()

# Definir la grid de hiperparámetros para ajustar
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [10, 20, 30],
    'criterion': ['gini', 'entropy']
}

# Implementar GridSearchCV para ajuste de hiperparámetros
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Mejor estimador encontrado por GridSearchCV
best_rf_model = grid_search.best_estimator_

print("Mejores hiperparámetros para RandomForestClassifier:")
print(grid_search.best_params_)

from sklearn.linear_model import LogisticRegression # Import LogisticRegression

# Utilizar el modelo Logistic Regression previo y el mejor modelo RandomForest en un ensamblador de votos
log_model = LogisticRegression(max_iter=1000)
voting_ensamble = VotingClassifier(estimators=[('lr', log_model), ('rf', best_rf_model)], voting='soft')

# Entrenar el ensamble con los datos de entrenamiento
voting_ensamble.fit(X_train, y_train)

# Evaluar el modelo de ensamblaje en el conjunto de prueba
y_pred_ensamble = voting_ensamble.predict(X_test)

# Métricas y gráficas para el modelo de ensamblaje

# Métricas de rendimiento adicionales
print("Informe de clasificación para el ensamble:")
print(classification_report(y_test, y_pred_ensamble, target_names=['Negative', 'Positive', 'Neutral']))

print("Exactitud del modelo de ensamblaje:")
accuracy_ensamble = accuracy_score(y_test, y_pred_ensamble)
print(accuracy_ensamble)

# Gráfico 1: Matriz de Confusión para el ensamble
cm_ensamble = confusion_matrix(y_test, y_pred_ensamble)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_ensamble, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive', 'Neutral'],
            yticklabels=['Negative', 'Positive', 'Neutral'])
plt.title('Matriz de Confusión del Ensamble')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Import necessary modules
from sklearn.metrics import roc_curve, auc

# Gráfico 2: Curva ROC para el ensamble
fpr_ensamble = {}
tpr_ensamble = {}
roc_auc_ensamble = {}

for i in range(3):
    fpr_ensamble[i], tpr_ensamble[i], _ = roc_curve(y_test, voting_ensamble.predict_proba(X_test)[:, i], pos_label=i)
    roc_auc_ensamble[i] = auc(fpr_ensamble[i], tpr_ensamble[i])

# Plotting ROC curve for each class
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue']
for i, color in zip(range(3), colors):
    plt.plot(fpr_ensamble[i], tpr_ensamble[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'
                                                     ''.format(i, roc_auc_ensamble[i]))
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curva multi-clase para Ensamble')
plt.legend(loc="lower right")
plt.show()

# API REST #


# Guardar el modelo entrenado y el vectorizador TF-IDF
import pickle

with open('logistic_regression_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

with open('tfidf_vectorizer.pkl', 'wb') as tfidf_file:
    pickle.dump(tfidf, tfidf_file)

print("El modelo y el vectorizador TF-IDF han sido guardados.")



import joblib

# Guardar el modelo entrenado
joblib.dump(model, 'logistic_regression_model.joblib')

# También puedes guardar el vectorizador TF-IDF de manera similar
joblib.dump(tfidf, 'tfidf_vectorizer.joblib')

# Cargar el modelo entrenado
model = joblib.load('logistic_regression_model.joblib')

# Cargar el vectorizador TF-IDF
tfidf = joblib.load('tfidf_vectorizer.joblib')

# Ejemplo de uso para hacer predicciones
def clean_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\d+', '', text)
    return text

# Suponiendo que deseas predecir una nueva revisión
new_review = "This is a bad app!"
cleaned_review = clean_text(new_review)
review_vector = tfidf.transform([cleaned_review])
prediction = model.predict(review_vector)

# Convertir resultado numérico a categoría
sentiment = 'Positive' if prediction[0] == 1 else 'Negative' if prediction[0] == 0 else 'Neutral'
print(sentiment)